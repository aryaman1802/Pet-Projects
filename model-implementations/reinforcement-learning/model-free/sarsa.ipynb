{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA (on-policy TD Control) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used while writing this notebook:\n",
    "- [Nimish Sanghi's book on Deep RL](https://www.amazon.com/Deep-Reinforcement-Learning-Python-TensorFlow/dp/1484268083)\n",
    "- [University of Alberta's RL specialization](https://www.coursera.org/specializations/reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SARSA](./algo_img/sarsa_on_policy_td_estimation.png \"SARSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe SARSA algorithm uses q-values (aka state-action values) instead of state values as used in TD(0) estimation algorithm:\n",
    "\n",
    "- TD Update:\n",
    "\n",
    "\\begin{align*}\n",
    "Q(S_t, A_t) \\; & = \\; Q(S_t, A_t) \\, + \\, \\alpha[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) \\, - \\, Q(S_t, A_t)] \\\\\n",
    "& = \\; Q(S_t, A_t) \\, + \\, \\alpha \\cdot \\delta_t \\\\\n",
    "\\end{align*}\n",
    "\n",
    ", where $\\; \\delta_t \\; = \\; R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) \\, - \\, Q(S_t, A_t) \\; = \\;$ TD error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out an update as per the above equation, we need all the five values $S_t$, $A_t$, $R_{t+1}$,\n",
    "$S_{t+1}$, and $A_{t+1}$. This is the reason the approach is called SARSA (state, action, reward,\n",
    "state, action)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please also note that for all episodic policies, the terminal states have $Q(S, A)$ equal to zero; i.e., once in terminal state, the person cannot transition anywhere and will keep getting a reward of zero. This is another way of saying that the episode ends and $Q(S, A)$ is zero for all terminal states. Therefore, when $S_{t+1}$ is a terminal state, the above equation will have $Q(S_{t+1}, A_{t+1}) \\, = \\, 0$, and the update equation will look like this:\n",
    "\n",
    "$$Q(S_t, A_t) \\; = \\; Q(S_t, A_t) \\, + \\, \\alpha[R_{t+1} \\, - \\, Q(S_t, A_t)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    \"\"\"SARSA (on-policy TD control) algorithm.\"\"\"\n",
    "    def __init__(self, get_possible_actions, policy, strategy, \n",
    "                 alpha=1, gamma=1, decay_rate=None):\n",
    "        self._Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.get_possible_actions = get_possible_actions\n",
    "        self.policy = policy\n",
    "        self.strategy = strategy\n",
    "        self.alpha = alpha   # learning rate or step size\n",
    "        self.gamma = gamma   # discount factor\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "\n",
    "    def _get_Q(self, state, action):\n",
    "        return self._Q[state][action]\n",
    "\n",
    "\n",
    "    def set_Q(self, state, action, value):\n",
    "        self._Q[state][action] = value\n",
    "\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"\n",
    "        This is the main update function for the SARSA algorithm.\n",
    "        \"\"\"\n",
    "        if not done:\n",
    "            td_error = reward + \\\n",
    "                       self.gamma * self._get_Q(next_state, next_action) - \\\n",
    "                       self._get_Q(state, action)\n",
    "        else:\n",
    "            td_error = reward - self._get_Q(state, action)\n",
    "        q_value = self._get_Q(state, action) + self.alpha * td_error\n",
    "        self.set_Q(state, action, q_value)\n",
    "        if self.decay_rate is not None:\n",
    "            self.alpha = self.alpha * self.decay_rate\n",
    "\n",
    "\n",
    "    def max_action(self, state):\n",
    "        \"\"\"\n",
    "        Return the best action for a given state, ie,\n",
    "        the action in the state-action pair that has the highest Q-value.\n",
    "        If there are multiple actions with the same Q-value,\n",
    "        return a random action from the set of best actions.\n",
    "        \"\"\"\n",
    "        actions = self.get_possible_actions(state)\n",
    "        best_action = []\n",
    "        best_q_value = float(\"-inf\")\n",
    "        for action in actions:\n",
    "            q_value = self._get_Q(state, action)\n",
    "            if q_value > best_q_value:\n",
    "                best_action = [action]\n",
    "                best_q_value = q_value\n",
    "            elif q_value == best_q_value:\n",
    "                best_action.append(action)\n",
    "        return np.random.choice(np.array(best_action))\n",
    "    \n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action based on the input strategy.\n",
    "        \"\"\"\n",
    "        return self.strategy(self.epsilon, self.get_possible_actions,\n",
    "                             self.state, self.max_action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_strategy(epsilon, get_possible_actions, state, max_action):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy strategy.\n",
    "\n",
    "    Choose a random number in the interval [0, 1) with a uniform \n",
    "    probability distribution. Use np.random.random() to do this.\n",
    "\n",
    "    If this random number is less than epsilon, return a random action.\n",
    "    Otherwise, return the best action for the given state.\n",
    "    \"\"\"\n",
    "    actions = get_possible_actions(state)\n",
    "    if len(actions) == 0:\n",
    "        return None\n",
    "    random_number = np.random.random()\n",
    "    if random_number < epsilon:\n",
    "        # exploration\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        # exploitation\n",
    "        return max_action(state)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out SARSA on Cliff Walking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gymnasium.farama.org/_images/cliff_walking.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://gymnasium.farama.org/_images/cliff_walking.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Cliff Walking environment\n",
    "cw_env = gym.make(\"CliffWalking-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, {'prob': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn():\n",
    "    q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out SARSA on Taxi environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gymnasium.farama.org/_images/taxi.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://gymnasium.farama.org/_images/taxi.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
