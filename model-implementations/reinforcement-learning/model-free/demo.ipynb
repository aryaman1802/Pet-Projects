{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used while writing this notebook:\n",
    "- [Nimish Sanghi's book on Deep RL](https://www.amazon.com/Deep-Reinforcement-Learning-Python-TensorFlow/dp/1484268083)\n",
    "- Tawsif Kamal's videos on Blackjack Monte Carlo Reinforcement Learning [part 1](https://youtu.be/NeusGkowXR4?si=9a1aE_bInK4vSAHw) and [part 2](https://youtu.be/wn8hlPNwL74?si=PV_h3WQCXmZKwRzW).\n",
    "- [Gymnasium docs for Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and enviroment setup\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create Blackjack environment\n",
    "env = gym.make(\"Blackjack-v1\", natural=False, sab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [this](https://gymnasium.farama.org/environments/toy_text/blackjack/) for more information about the Blackjack environment. \n",
    "\n",
    "In addition, check [this](https://youtu.be/NeusGkowXR4?si=9a1aE_bInK4vSAHw) and [this](https://youtu.be/wn8hlPNwL74?si=PV_h3WQCXmZKwRzW) videos for how the below code is written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "\n",
    "- 0: Stick\n",
    "- 1: Hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting State\n",
    "\n",
    "| **Observation** | **Values** |\n",
    "| --- | --- |\n",
    "| Player current sum | $4, 5, \\cdots, 21$ |\n",
    "| Dealer showing card value | $1, 2, \\cdots, 11$ |\n",
    "| Usable Ace | $0, 1$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "- win game: +1\n",
    "- lose game: -1\n",
    "- draw game: 0\n",
    "- win game with natural blackjack: +1.5 (if `natural` is True) +1 (if `natural` is False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode End\n",
    "\n",
    "The episode ends if the following happens:\n",
    "\n",
    "- Termination:\n",
    "\n",
    "1. The player hits and the sum of hand exceeds 21.\n",
    "2. The player sticks.\n",
    "\n",
    "An ace will always be counted as usable (11) unless it busts the player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Q function will map 3 states to 2 actions, ie, $\\text{Q}: \\; 3 \\text{ states} \\; \\rightarrow \\; 2 \\text{ actions}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [this](https://www.geeksforgeeks.org/defaultdict-in-python/) for a refresher on `defaultdict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of different discrete actions\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(32), Discrete(11), Discrete(2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 10, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.reset()\n",
    "env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q function takes as input the 3 different state values:\n",
    "- player's current\n",
    "- dealer's first card\n",
    "- whether the player has a usable ace\n",
    "\n",
    "Based on these 3 state values, the Q function returns 2 action values:\n",
    "- value of action 0, ie, the action-value of playing 'stick'.\n",
    "- value of action 1, ie, the action-value of playing 'hit'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[env.reset()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_soft_policy(Q, state, epsilon):\n",
    "    probs = np.zeros(2)\n",
    "    # since there are only two actions: 0 and 1\n",
    "    # so, optimal action is either 0 or 1\n",
    "    # and, sub-optimal action is the other one \n",
    "    optimal_action = np.argmax(Q[state])\n",
    "    sub_optimal_action = np.abs(optimal_action - 1)\n",
    "    probs[optimal_action] = 1 - epsilon + epsilon/env.action_space.n\n",
    "    probs[sub_optimal_action] = epsilon/env.action_space.n\n",
    "    action = np.random.choice([0,1], p=probs)\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(Q, epsilon):\n",
    "    state, _ = env.reset()\n",
    "    episode = []\n",
    "    while True:\n",
    "        action = epsilon_soft_policy(Q, state, epsilon)\n",
    "        next_state, reward, done, trunc, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if (done or trunc):\n",
    "            break\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> generate_episode(Q, 0.7)\n",
    "[((16, 10, 1), 1, 0.0), ((16, 10, 0), 0, -1.0)]\n",
    "```\n",
    "\n",
    "The output of the above code can be interpreted as follows:\n",
    "-  ((16, 10, 1), 1, 0.0)\n",
    "    - This tuple consists of 3 elements:\n",
    "        1. (16, 10, 1) is a tuple where the 1st element (ie, 16) is the player's current sum, 2nd element (ie, 10) is the dealer's open card value, and 3rd element (ie, 1) tells whether the player has a usable ace or not.\n",
    "        2. 1 is the action that the player takes\n",
    "        3. 0.0 is the reward that the player gets after taking the above action (ie, action 1)\n",
    "-  ((16, 10, 0), 0, -1.0)\n",
    "    - This tuple consists of 3 elements:\n",
    "        1. (16, 10, 0) is a tuple where the 1st element (ie, 16) is the player's current sum, 2nd element (ie, 10) is the dealer's open card value, and 3rd element (ie, 0) tells whether the player has a usable ace or not.\n",
    "        2. 0 is the action that the player takes\n",
    "        3. -1.0 is the reward that the player gets after taking the above action (ie, action 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((14, 10, 0), 0, -1.0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_episode(Q, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {(15, 7, 1): array([0., 0.]),\n",
       "             (17, 9, 0): array([0., 0.]),\n",
       "             (8, 4, 0): array([0., 0.]),\n",
       "             (16, 10, 1): array([0., 0.]),\n",
       "             (16, 10, 0): array([0., 0.]),\n",
       "             (14, 10, 0): array([0., 0.])})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kamal_MC:\n",
    "    def __init__(self, num_episodes, epsilon, epsilon_min, alpha, decay_rate, gamma):\n",
    "        \"\"\"\n",
    "        param num_episodes: (int) number of episodes to run the algorithm\n",
    "        param epsilon: (float) \n",
    "        param epsilon_min: (float) \n",
    "        param alpha: (float)\n",
    "        param decay_rate: (float)\n",
    "        param gamma: (float) discount factor\n",
    "        \"\"\"\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.alpha = alpha\n",
    "        self.decay_rate = decay_rate\n",
    "        self.gamma = gamma\n",
    "        self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.policy = {}\n",
    "        self.rewards_all_episodes = []\n",
    "\n",
    "    def monte_carlo_control(self):\n",
    "        for episode in tqdm(range(1, self.num_episodes + 1)):\n",
    "            # gradually decrease epsilon until it reaches epsilon_min\n",
    "            # then keep it constant, i.e., epsilon_min\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.decay_rate)\n",
    "            experience = generate_episode(self.Q, self.epsilon)\n",
    "            states, actions, rewards = zip(*experience)\n",
    "            rewards = np.array(rewards)\n",
    "            self.rewards_all_episodes.append(rewards.sum())\n",
    "            # looping over all of the timesteps \n",
    "            for i, state in enumerate(states):\n",
    "                discounts = np.array([self.gamma**j for j in range(len(rewards[i: ]))])\n",
    "                returns = sum(rewards[i: ] * discounts)\n",
    "                # updating the Q_function using the monte carlo constant alpha update rule \n",
    "                self.Q[state][actions[i]] += self.alpha * (returns - self.Q[state][actions[i]])\n",
    "                self.policy = dict((state, np.argmax(q_value)) for state, q_value in self.Q.items())\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 7, 10, 13, 16)\n",
      "(2, 5, 8, 11, 14, 17)\n",
      "(3, 6, 9, 12, 15, 18)\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    experience = [(1,2,3), (4,5,6), (7,8,9), (10,11,12), (13,14,15), (16,17,18)]\n",
    "    states, actions, rewards = zip(*experience)\n",
    "    print(states)\n",
    "    print(actions)\n",
    "    print(rewards)\n",
    "\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_episodes = 10_000\n",
    "epsilon = 1\n",
    "epsilon_min = 0.05\n",
    "alpha = 0.03\n",
    "decay_rate = 0.9999\n",
    "gamma = 1\n",
    "\n",
    "mcc = Kamal_MC(num_episodes, epsilon, epsilon_min, alpha, decay_rate, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 827.59it/s]\n"
     ]
    }
   ],
   "source": [
    "mcc.monte_carlo_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2920.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mcc.rewards_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLIE_MC_Control:\n",
    "    def __init__(self, num_episodes, epsilon, epsilon_min=0.05, decay_rate=0.9999, gamma=1):\n",
    "        \"\"\"\n",
    "        param num_episodes: (int) number of episodes to run the algorithm\n",
    "        param epsilon: (float) \n",
    "        param epsilon_min: (float)\n",
    "        param decay_rate: (float)\n",
    "        param gamma: (float) discount factor\n",
    "        \"\"\"\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.decay_rate = decay_rate\n",
    "        self.gamma = gamma\n",
    "        self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.visit_count = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.policy = defaultdict(lambda: 0)\n",
    "        self.rewards_all_episodes = []\n",
    "\n",
    "\n",
    "    def every_visit(self):\n",
    "        \"\"\"\n",
    "        This is the every-visit GLIE Monte Carlo Control algorithm.\n",
    "        Check pg. 88 of Nimish Sanghi's book for the pseudocode.\n",
    "        \"\"\"\n",
    "        for episode in tqdm(range(1, self.num_episodes+1)):\n",
    "            experience = generate_episode(self.Q, self.epsilon)\n",
    "            states, actions, rewards = zip(*experience)\n",
    "            self.rewards_all_episodes.append(sum(rewards))\n",
    "            G = 0\n",
    "            T = len(states)\n",
    "            for t in range(T-1, -1, -1):\n",
    "                s, a, r = states[t], actions[t], rewards[t]\n",
    "                G = self.gamma*G + r\n",
    "                self.visit_count[s][a] += 1\n",
    "                self.Q[s][a] = self.Q[s][a] + (1 / self.visit_count[s][a]) * (G - self.Q[s][a])\n",
    "                for state_value, q_value in self.Q.items():\n",
    "                    self.policy[state_value] = np.argmax(q_value)\n",
    "            self.epsilon = self.epsilon/episode\n",
    "\n",
    "\n",
    "    def first_visit(self):\n",
    "        \"\"\"\n",
    "        This is the first-visit GLIE Monte Carlo Control algorithm.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_episodes = 20_000\n",
    "epsilon = 1\n",
    "epsilon_min = 0.05\n",
    "decay_rate = 0.9999\n",
    "gamma = 1\n",
    "\n",
    "ev_mcc = GLIE_MC_Control(num_episodes, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:20<00:00, 995.99it/s] \n"
     ]
    }
   ],
   "source": [
    "ev_mcc.every_visit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3225.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ev_mcc.rewards_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
