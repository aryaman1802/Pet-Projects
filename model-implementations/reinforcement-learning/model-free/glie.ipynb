{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLIE MC Control Algorithm\n",
    "\n",
    "- GLIE $\\; \\rightarrow \\;$ Greedy in the Limit with Infinite Exploration\n",
    "- MC $\\; \\rightarrow \\;$ Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used while writing this notebook:\n",
    "- [Nimish Sanghi's book on Deep RL](https://www.amazon.com/Deep-Reinforcement-Learning-Python-TensorFlow/dp/1484268083)\n",
    "- Tawsif Kamal's videos on Blackjack Monte Carlo Reinforcement Learning [part 1](https://youtu.be/NeusGkowXR4?si=9a1aE_bInK4vSAHw) and [part 2](https://youtu.be/wn8hlPNwL74?si=PV_h3WQCXmZKwRzW).\n",
    "- [Gymnasium docs for Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_soft_policy(env, Q, state, epsilon):\n",
    "    probs = np.zeros(2)\n",
    "    # since there are only two actions: 0 and 1\n",
    "    # so, optimal action is either 0 or 1\n",
    "    # and, sub-optimal action is the other one \n",
    "    optimal_action = np.argmax(Q[state])\n",
    "    sub_optimal_action = np.abs(optimal_action - 1)\n",
    "    probs[optimal_action] = 1 - epsilon + epsilon/env.action_space.n\n",
    "    probs[sub_optimal_action] = epsilon/env.action_space.n\n",
    "    action = np.random.choice([0,1], p=probs)\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, Q, epsilon):\n",
    "    state, _ = env.reset()\n",
    "    episode = []\n",
    "    while True:\n",
    "        action = epsilon_soft_policy(Q, state, epsilon)\n",
    "        next_state, reward, done, trunc, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if (done or trunc):\n",
    "            break\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLIE_MC_Control:\n",
    "    def __init__(self, env, num_episodes, epsilon, epsilon_min=0.05, decay_rate=0.9999, gamma=1):\n",
    "        \"\"\"\n",
    "        param env: (gymnasium environment) the environment to run the algorithm on\n",
    "        param num_episodes: (int) number of episodes to run the algorithm\n",
    "        param epsilon: (float) \n",
    "        param epsilon_min: (float)\n",
    "        param decay_rate: (float)\n",
    "        param gamma: (float) discount factor\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.decay_rate = decay_rate\n",
    "        self.gamma = gamma\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.env.action_space.n))\n",
    "        self.visit_count = defaultdict(lambda: np.zeros(self.env.action_space.n))\n",
    "        self.policy = defaultdict(lambda: 0)\n",
    "        self.rewards_all_episodes = []\n",
    "\n",
    "\n",
    "    def every_visit(self):\n",
    "        \"\"\"\n",
    "        This is the every-visit GLIE Monte Carlo Control algorithm.\n",
    "        Check pg. 88 of Nimish Sanghi's book for the pseudocode.\n",
    "        \"\"\"\n",
    "        for episode in tqdm(range(1, self.num_episodes+1)):\n",
    "            experience = generate_episode(self.Q, self.epsilon)\n",
    "            states, actions, rewards = zip(*experience)\n",
    "            self.rewards_all_episodes.append(sum(rewards))\n",
    "            G = 0\n",
    "            T = len(states)\n",
    "            for t in range(T-1, -1, -1):\n",
    "                s, a, r = states[t], actions[t], rewards[t]\n",
    "                G = self.gamma*G + r\n",
    "                self.visit_count[s][a] += 1\n",
    "                self.Q[s][a] = self.Q[s][a] + (1 / self.visit_count[s][a]) * (G - self.Q[s][a])\n",
    "                for state_value, q_value in self.Q.items():\n",
    "                    self.policy[state_value] = np.argmax(q_value)\n",
    "            self.epsilon = self.epsilon/episode\n",
    "\n",
    "\n",
    "    def first_visit(self):\n",
    "        \"\"\"\n",
    "        This is the first-visit GLIE Monte Carlo Control algorithm.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
