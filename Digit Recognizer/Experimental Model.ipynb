{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-22T12:14:58.689058Z",
     "iopub.status.busy": "2022-05-22T12:14:58.687995Z",
     "iopub.status.idle": "2022-05-22T12:14:59.70609Z",
     "shell.execute_reply": "2022-05-22T12:14:59.705351Z",
     "shell.execute_reply.started": "2022-05-22T12:14:58.688947Z"
    },
    "papermill": {
     "duration": 1.286982,
     "end_time": "2022-05-16T10:02:11.767224",
     "exception": false,
     "start_time": "2022-05-16T10:02:10.480242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 01:19:52.155108: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-12 01:19:52.155138: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,202\n",
      "Trainable params: 104,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 01:19:57.261287: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-12 01:19:57.261392: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aryaman-Surface-Book-2): /proc/driver/nvidia/version does not exist\n",
      "2022-06-12 01:19:57.262361: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(28,28,1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 19s 19ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0391 - val_accuracy: 0.9913\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9926\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0377 - val_accuracy: 0.9927\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0475 - val_accuracy: 0.9912\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0381 - val_accuracy: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd6c9d84d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = \"train_data_checkpoint_path.keras\",\n",
    "        monitor = 'val_loss',\n",
    "        save_best_only = True)\n",
    "]\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, \n",
    "          callbacks=my_callback, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0381 - accuracy: 0.9923\n",
      "Test Accuracy:\t 0.992\n",
      "Test Loss:\t 0.038\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy:\\t {test_acc:.3f}\")\n",
    "print(f\"Test Loss:\\t {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the full dataset (`train` + `test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 28, 28, 1), (10000,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_images = np.concatenate((train_images, test_images))\n",
    "total_labels = np.concatenate((train_labels, test_labels))\n",
    "total_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1094/1094 [==============================] - 21s 19ms/step - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 2/10\n",
      "1094/1094 [==============================] - 22s 20ms/step - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 3/10\n",
      "1094/1094 [==============================] - 21s 19ms/step - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 4/10\n",
      "1094/1094 [==============================] - 21s 19ms/step - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 5/10\n",
      "1094/1094 [==============================] - 22s 20ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 6/10\n",
      "1094/1094 [==============================] - 21s 19ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 7/10\n",
      "1094/1094 [==============================] - 21s 20ms/step - loss: 0.0019 - accuracy: 0.9994\n",
      "Epoch 8/10\n",
      "1094/1094 [==============================] - 23s 21ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 9/10\n",
      "1094/1094 [==============================] - 21s 19ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 10/10\n",
      "1094/1094 [==============================] - 21s 19ms/step - loss: 0.0021 - accuracy: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd6c9384bb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_callback2 = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "    filepath = \"total_data_checkpoint_path.keras\",\n",
    "    monitor = 'loss',\n",
    "    save_best_only = True)\n",
    "]\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(total_images, total_labels, epochs=10, batch_size=64, \n",
    "          callbacks=my_callback2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 22:52:20.541022: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://30e52e65-d6c6-420a-81cf-d133ac680dba/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['convnet_for_mnist.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, \"convnet_for_mnist.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 3.7286e-04 - accuracy: 0.9998\n",
      "Test Accuracy:\t 1.000\n",
      "Test Loss:\t 0.000\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load(\"convnet_for_mnist.joblib\")\n",
    "lm_test_loss, lm_test_acc = loaded_model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy:\\t {lm_test_acc:.3f}\")\n",
    "print(f\"Test Loss:\\t {lm_test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomZoom(0.3)       # 30%\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 4\n",
    "digit = train_images[index]\n",
    "plt.imshow(digit, cmap=plt.get_cmap('gray'))\n",
    "plt.title(f\"Label: {train_labels[index]}\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "augmented_digit = data_augmentation(digit).numpy()\n",
    "plt.imshow(augmented_digit, cmap=plt.get_cmap('gray'))\n",
    "plt.title(f\"Label: {train_labels[index]}\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
